10.分析海量数据
	a.模拟 海量数据 存储过程(无return)/函数(有return)
		创建数据库已经数据表
			create database testdata;
			use testdata;
			create table dept(
				id int primary key,
				name varchar(20) not null default '',
				loc varchar(30) not null default ''
			)engine=innodb default charset=utf8;
			create table emp(
				id int primary key,
				name varchar(20) not null default '',
				job varchar(20) not null default '',
				deptid int not null default 0
			)engine=innodb default charset=utf8;		
			通过存储函数插入海量数据：
		创建存储函数：
			rendstring(6)->用于模拟员工名称
			delimiter $	
			create function randstring(n int)	returns varchar(255)
			begin
				declare default all_str varchar(100) default 'abcdefghijklmnopqrstuvwxyz';
				declare return_str varchar(255) default '';
				declare i int default 0;
				while i<n
					do
						set return_str=concat(return_str,substring(all_str,floor(1+rand()*26,1)));
						set i=i+1;
					end while;
				return return_str;
			end $
			如果报错syntax，说明sql语句语法有错，需要修改sql语句
			
			存储过程和存储函数在创建时与之前开启的慢查询日志冲突了。（即开启慢查询日志和创建冲突了）
			解决冲突：
			临时解决(需要开启log_bin_trust_function_creators)
				show variables like '%log_bin_trust_function_creators%'
				set global log_bin_trust_function_creators=1
				show variables like '%log_bin_trust_function%'
				+---------------------------------+-------+
				| Variable_name                   | Value |
				+---------------------------------+-------+
				| log_bin_trust_function_creators | ON    |
				+---------------------------------+-------+
			永久解决：
				/etc/my.cnf
					[mysqld]
						log_bin_trust_function_creators=1
			产生随机整数				
			delimiter $	
			create function ran_num()	returns int(5)
			begin
				declare i int default 0;
				set i=floor(rand()*100);
				return i;
			end $						
		通过存储过程插入海量数据，emp表中	
			create procedure insert_emp(in id_start int(10),in data_times int(10))
			begin
				declare i int default 0;
				set autocommit=0;	关闭自动提交，提高效率，最后批量提交
				repeat
					insert into emp values(id_start+i,randstring(5),'job',ran_num());
					set i=i+1;
					until i=data_times
				end repeat;
			end $
		通过存储过程插入海量数据，dept表中	
			create procedure insert_dept(in id_start int(10),in data_times int(10))
			begin
				declare i int default 0;
				set autocommit=0;	关闭自动提交，提高效率，最后批量提交
				repeat
					insert into dept values(id_start+i,randstring(6),randstring(8));
					set i=i+1;
					until i=data_times
				end repeat;
			end $						
		插入数据
			truncate table emp;
			delimiter ;
			call insert_emp(0,800000);
			call insert_dept(0,30);
			
【ck】以上都是创建海量数据，以下开始分析海量数据。			
	b.分析海量数据
		show profiles;	默认是关闭的
		show variables like 'profiling'
		set profiling = on;
		show profiles;
		show variables like '%profiling%'
		+------------------------+-------+
		| Variable_name          | Value |
		+------------------------+-------+
		| have_profiling         | YES   |
		| profiling              | OFF   |
		| profiling_history_size | 15    |
		+------------------------+-------+
		set profiling=on;
		show variables like '%profiling%';
		+------------------------+-------+
		| Variable_name          | Value |
		+------------------------+-------+
		| have_profiling         | YES   |
		| profiling              | ON    |
		| profiling_history_size | 15    |
		+------------------------+-------+
		show profiles:会记录所有show profiles打开后执行的sql所花费的时间（即需要set profiling = on;把profiling打开）
			不够精确io、cpu、内存，是总攻的。
		精确分析：sql诊断
			show profile all for query 上一步查询到的queryid
			show profile cpu,io for query	上一步查询到的queryid			
			show profiles;
			+----------+------------+---------------------------+
			| Query_ID | Duration   | Query                     |
			+----------+------------+---------------------------+
			|        1 | 0.09224725 | select count(*) from emp  |
			|        2 | 0.05622625 | SELECT DATABASE()         |
			|        3 | 0.16921975 | select count(*) from dept |
			|        4 | 0.00021225 | set profiling=on          |
			|        5 | 0.00014525 | select count(*) from dept |
			+----------+------------+---------------------------+
			show profile cpu for query 5;
			+--------------------------------+----------+----------+------------+
			| Status                         | Duration | CPU_user | CPU_system |
			+--------------------------------+----------+----------+------------+
			| starting                       | 0.000045 | 0.000000 |   0.000000 |
			| Waiting for query cache lock   | 0.000013 | 0.000000 |   0.000000 |
			| checking query cache for query | 0.000015 | 0.000000 |   0.000000 |
			| checking privileges on cached  | 0.000011 | 0.000000 |   0.000000 |
			| checking permissions           | 0.000017 | 0.000000 |   0.000000 |
			| sending cached result to clien | 0.000025 | 0.000000 |   0.000000 |
			| logging slow query             | 0.000011 | 0.000000 |   0.000000 |
			| cleaning up                    | 0.000010 | 0.000000 |   0.000000 |
			+--------------------------------+----------+----------+------------+
			show profile cpu,block io for query 5;
			+--------------------------------+----------+----------+------------+--------------+---------------+
			| Status                         | Duration | CPU_user | CPU_system | Block_ops_in | Block_ops_out |
			+--------------------------------+----------+----------+------------+--------------+---------------+
			| starting                       | 0.000045 | 0.000000 |   0.000000 |            0 |             0 |
			| Waiting for query cache lock   | 0.000013 | 0.000000 |   0.000000 |            0 |             0 |
			| checking query cache for query | 0.000015 | 0.000000 |   0.000000 |            0 |             0 |
			| checking privileges on cached  | 0.000011 | 0.000000 |   0.000000 |            0 |             0 |
			| checking permissions           | 0.000017 | 0.000000 |   0.000000 |            0 |             0 |
			| sending cached result to clien | 0.000025 | 0.000000 |   0.000000 |            0 |             0 |
			| logging slow query             | 0.000011 | 0.000000 |   0.000000 |            0 |             0 |
			| cleaning up                    | 0.000010 | 0.000000 |   0.000000 |            0 |             0 |
			+--------------------------------+----------+----------+------------+--------------+---------------+


						
						
						
						
						
						
						
						
						
						
						
						
						
						
						
						
		